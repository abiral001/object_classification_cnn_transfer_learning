{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa241353",
   "metadata": {},
   "source": [
    "## Object Detection Comparison between multiple main stream models\n",
    "\n",
    "# Steps to Reproduce\n",
    "\n",
    "1. Take your train and test images and place it into the root directory of this project. \\n\n",
    "    The folder structure should be the one shown below:\n",
    "    \\n\n",
    "        --- 01-classify.ipynb\n",
    "         |- 02-final-test.ipynb\n",
    "         |- test_images/\n",
    "         |          |- test-images/\n",
    "         |          |       |- images ... (.jpg)\n",
    "         |- train_images/\n",
    "         |          |- train-images/\n",
    "         |          |       |- images ... (.jpg)\n",
    "         |- models/\n",
    "         |- inet/\n",
    "         |- rnet/\n",
    "         |- enet/\n",
    "         |- train-labels.csv\n",
    "    \\n\n",
    "\n",
    "2. Install the python packages mentioned in requirements.txt either through pip directly or using virtual environment (recommended)\\n\n",
    "3. Run the classify.ipynb for generating the model using each of the algorithm (each of the algorithm should be commented out before runnning another one)\n",
    "   The Trained model will have each of its checkpoint saved inside the /models/ folder. Copy the final model into the respective model folder (inet for Inception Net, rnet for ResNet50 and enet for Efficient Net) [NOTE: the model is trained using train set and validation set generated through the images inside train_images]\\n\n",
    "4. Run the final-test.ipynb to test the images from the folder test_images. The predictions are saved in the root directory as predictions.pt as a sequence of categories sequentially corresponding to the test-files sorted by ascending order.\\n\n",
    "\n",
    "This file content will also be placed inside the classify.ipynb file for instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec8ea4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831e0306-6046-42cc-84ed-a8d389b728c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = pd.read_csv('./train-labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b579499-3b60-4298-9cf3-84f70242c97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# proper_labels = pd.DataFrame()\n",
    "# for image in [x for x in os.listdir('./train-files/train_files/') if x.endswith(\".jpg\")]:\n",
    "#     file_index = int(image.split(\".jpg\")[0].split(\"-\")[1])\n",
    "#     category_index = labels.iloc[file_index]['cat']\n",
    "#     row = {\n",
    "#         'filename': image,\n",
    "#         'file_index': file_index,\n",
    "#         'category': category_index\n",
    "#     }\n",
    "#     proper_labels = pd.concat([proper_labels, pd.DataFrame([row])], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db27395-c43e-4091-8f9b-a1db186817c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# proper_labels.to_csv('./proper_labels.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0220ff-fcdc-444f-a443-27c8e02a4bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "proper_labels = pd.read_csv('./proper_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bdbf4b-54d9-4b4b-96a4-5590e33bcf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "proper_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d928a1e-d723-43d5-8366-ffcedc8501c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting total number of classes\n",
    "print(pd.DataFrame(proper_labels[['category', 'filename']].groupby('category').count()).reset_index())\n",
    "total_categories = len(proper_labels.category.unique())\n",
    "print(\"Total categories: \", total_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6abd9d-0fb4-46ab-9952-c2ea0f578f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dcf69f-5ff8-4d5a-b217-d897fb121461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision.transforms import v2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c6a3e6-6c7a-4fe9-99d1-49a7d8f803c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10cd443-87ca-478e-b733-2bf1a5dce83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# efficient net\n",
    "image_width = 300 \n",
    "image_height = 200\n",
    "# inception net\n",
    "# image_width = 299 \n",
    "# image_height = 299\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Inferencing and training through: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a328593a-49f8-412b-a487-ac0af978bcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmentation and normalization with convertion to tensor\n",
    "preprocess_image = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.RandomResizedCrop(size=(image_width, image_height), antialias=True),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.RandomVerticalFlip(p=0.5),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean, std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1aaeed5-ffef-4907-8779-71908613ea2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for im_name in [x for x in os.listdir('./train-files/train_files/') if x.endswith(\".jpg\")]:\n",
    "    image = cv2.resize(\n",
    "        cv2.cvtColor(\n",
    "            cv2.imread(\n",
    "                f'./train-files/train_files/{im_name}', \n",
    "                cv2.IMREAD_COLOR\n",
    "            ), \n",
    "            cv2.COLOR_BGR2RGB\n",
    "        ), \n",
    "        (image_width, image_height)\n",
    "    ) # read image, convert color space, resize all to (300, 200)\n",
    "    # preprocessing data images\n",
    "    image = preprocess_image(image).to(device)\n",
    "    # embedding the category\n",
    "    category = torch.tensor(\n",
    "        [proper_labels[proper_labels['filename'] == im_name].iloc[0]['category']]\n",
    "    ).to(device)\n",
    "    dataset.append((image, category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2989a5a8-f8d3-42ca-a6a3-9adf11903e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ff2c0e-e802-406b-98a5-9c52d29c94a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset = ClassificationDataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91de8ff-5c14-42d4-bf1b-c2fa339b62f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, validate_dataset = random_split(final_dataset, [0.85, 0.15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2452cea-d3d1-44ed-ba2d-cc175cabe818",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486f244f-a4bc-4e32-9942-d5d87cbcdbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "testloader = DataLoader(validate_dataset, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c3bd66-8c5c-4edf-a1c1-0db870b9f263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# efficient net v2_l\n",
    "model = models.efficientnet_v2_l(weights = 'EfficientNet_V2_L_Weights.DEFAULT')\n",
    "# model = models.efficientnet_v2_l(weights = None)\n",
    "# checking the head's name\n",
    "for name, module in model.named_modules():\n",
    "    if name.startswith('classifier'):\n",
    "        print(name, module)\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(p = 0.5),\n",
    "    nn.Linear(1280, total_categories, bias = True),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")\n",
    "# load saved checkpoint\n",
    "# model.load_state_dict(torch.load('./enet/en3_10.pth'))\n",
    "# multi gpu optimizations\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model, device_ids = [0, 1])\n",
    "    final_layer = model.module.classifier\n",
    "else:\n",
    "    final_layer = model.classifier\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7554bb4-7482-4b54-95c3-57130e311c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # inception net\n",
    "# model = models.inception_v3(weights = 'Inception_V3_Weights.DEFAULT')\n",
    "# # checking the head's name\n",
    "# for name, module in model.named_modules():\n",
    "#     if name.startswith('fc'):\n",
    "#         print(name, module)\n",
    "# model.fc = nn.Sequential(\n",
    "#     nn.Linear(2048, total_categories, bias = True),\n",
    "#     nn.LogSoftmax(dim=1)\n",
    "# )\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#     model = nn.DataParallel(model, device_ids = [0, 1])\n",
    "#     final_layer = model.module.fc\n",
    "# else:\n",
    "#     final_layer = model.fc\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb13d7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # resnet50\n",
    "# model = models.resnet152(weights='ResNet152_Weights.DEFAULT')\n",
    "# # checking the head's name\n",
    "# for name, module in model.named_modules():\n",
    "#     if name.startswith('fc'):\n",
    "#         print(name, module)\n",
    "# model.classifier = nn.Sequential(\n",
    "#     nn.Linear(2048, total_categories, bias = True),\n",
    "#     nn.LogSoftmax(dim=1)\n",
    "# )\n",
    "# # load saved checkpoint\n",
    "# # model.load_state_dict(torch.load('./enet/en3_10.pth'))\n",
    "# # multi gpu optimizations\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#     model = nn.DataParallel(model, device_ids = [0, 1])\n",
    "#     final_layer = model.module.fc\n",
    "# else:\n",
    "#     final_layer = model.fc\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d772cb5c-fb27-43d8-bcac-ab5fe23d4721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # custom model from scratch\n",
    "# class Classifier(nn.Module):\n",
    "#     def __init__(self, in_channels):\n",
    "#         super(Classifier, self).__init__()\n",
    "#         self.layer1 = nn.Conv2d(in_channels, 32, 5, 2, 2)\n",
    "#         self.lrelu = nn.LeakyReLU()\n",
    "#         self.dropout = nn.Dropout(0.2)\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.layer2 = nn.Conv2d(32, 64, 3, 1, 2)\n",
    "#         self.layer3 = nn.Conv2d(64, 128, 3, 1, 2)\n",
    "#         self.layer4 = nn.Conv2d(128, 1, 4, 2, 2)\n",
    "#         self.layer5 = nn.Linear(4134, 1000)\n",
    "#         self.output = nn.Linear(1000, total_categories)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.lrelu(self.layer1(x))\n",
    "#         x = self.layer2(x)\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.layer3(x)\n",
    "#         x = self.lrelu(self.layer4(x))\n",
    "#         x = self.flatten(x)\n",
    "#         x = self.layer5(x)\n",
    "#         x = self.output(x)\n",
    "#         return F.log_softmax(x, dim = 1)\n",
    "# model = Classifier(3)\n",
    "# # # multi gpu optimizations\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#     model = nn.DataParallel(model, device_ids = [0, 1])\n",
    "# model = model.to(device)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ea9522-7fa0-4384-a2f8-cef2a94cee06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "        model, model_layer, \n",
    "        trainloader, testloader, \n",
    "        learning_rate, weights_decay, \n",
    "        epochs, train_transferred = False, \n",
    "        name = '', run = None\n",
    "):\n",
    "    if model_layer is None:\n",
    "        model_layer = model\n",
    "    for params in model.parameters():\n",
    "        params.requires_grad = train_transferred\n",
    "    for params in model_layer.parameters():\n",
    "        params.requires_grad = True\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate, weight_decay=weights_decay)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        for idx, (train_images, train_labels) in enumerate(trainloader):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(train_images)\n",
    "\n",
    "            # # for all others\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            loss = loss_function(outputs, train_labels.squeeze())\n",
    "            # # only for inception net\n",
    "            # _, predicted = torch.max(outputs[0], 1)\n",
    "            # loss = loss_function(outputs[0], train_labels.squeeze())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total = train_labels.size(0)\n",
    "            correct = (predicted == train_labels.squeeze()).sum().item()\n",
    "            accuracy = (correct*100.0/total)\n",
    "            if run is not None:\n",
    "                run.log({\n",
    "                    'iteration': idx+len(trainloader)*(epoch-1),\n",
    "                    'train_loss': loss,\n",
    "                    'train_accuracy': accuracy,\n",
    "                })\n",
    "            if idx%50 == 0 or idx == len(trainloader)-1:\n",
    "                print(f\"Epoch {epoch}, iteration: {idx}/{len(trainloader)}, loss: {loss}\")\n",
    "        # validate \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            loss_list = list()\n",
    "            for (val_images, val_labels) in testloader:\n",
    "                outputs = model(val_images)\n",
    "                \n",
    "                # # for all others\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                loss_list.append(loss_function(outputs, val_labels.squeeze()).detach().cpu().numpy())\n",
    "                # # only for inception net\n",
    "                # _, predicted = torch.max(outputs, 1)\n",
    "                # loss_list.append(loss_function(outputs, val_labels.squeeze()).detach().cpu().numpy())\n",
    "\n",
    "                total += val_labels.size(0)\n",
    "                correct += (predicted == val_labels.squeeze()).sum().item()\n",
    "        accuracy = (correct*100.0/total)\n",
    "        if run is not None:\n",
    "            run.log({\n",
    "                'epoch': epoch, \n",
    "                'val_accuracy': accuracy,\n",
    "                'val_loss': np.average(loss_list)\n",
    "            })\n",
    "        print(f\"Validation after Epoch: {epoch}, Accuracy: \", accuracy)\n",
    "        if name != '':\n",
    "            torch.save(model.state_dict(), f'./models/{name}_{epoch}.pth')\n",
    "        else:\n",
    "            torch.save(model.state_dict(), f'./models/model_{epoch}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fbdff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # custom\n",
    "# epochs = 20\n",
    "# learning_rate = 0.001\n",
    "# weights_decay = 0.00001\n",
    "# run = wandb.init(\n",
    "#     project=\"assignment-3\",\n",
    "#     name=f\"custom-{learning_rate}-head_only\",\n",
    "#     config={\n",
    "#         \"learning_rate\": learning_rate,\n",
    "#         \"architecture\": \"Custom CNN\",\n",
    "#         \"dataset\": \"Custom\",\n",
    "#         \"epochs\": epochs,\n",
    "#     }\n",
    "# )\n",
    "# run.watch(model, log='all', log_graph=True)\n",
    "# train_model(\n",
    "#     model,\n",
    "#     None,\n",
    "#     trainloader, \n",
    "#     testloader, \n",
    "#     learning_rate, \n",
    "#     weights_decay,\n",
    "#     epochs, \n",
    "#     train_transferred = False, \n",
    "#     run = run, \n",
    "#     name=\"pt_no\"\n",
    "# )\n",
    "# run.finish()\n",
    "# # retraining the entire model with lower learning rate\n",
    "# epochs = 20\n",
    "# learning_rate = 0.0001\n",
    "# run = wandb.init(\n",
    "#     project=\"assignment-3\",\n",
    "#     name=f\"custom-{learning_rate}-all\",\n",
    "#     config={\n",
    "#         \"learning_rate\": learning_rate,\n",
    "#         \"architecture\": \"Custom CNN\",\n",
    "#         \"dataset\": \"Custom\",\n",
    "#         \"epochs\": epochs,\n",
    "#     }\n",
    "# )\n",
    "# train_model(\n",
    "#     model,\n",
    "#     None,\n",
    "#     trainloader, \n",
    "#     testloader, \n",
    "#     learning_rate, \n",
    "#     weights_decay,\n",
    "#     epochs, \n",
    "#     train_transferred = True, \n",
    "#     run = run, \n",
    "#     name=\"pt_yes\"\n",
    "# )\n",
    "# run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a65f572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# efficientnet_v2_l\n",
    "epochs = 25\n",
    "learning_rate = 0.001\n",
    "weights_decay = 0.00001\n",
    "run = wandb.init(\n",
    "    project=\"assignment-3\",\n",
    "    name=f\"enet-b{batch_size}-{learning_rate}-head_only\",\n",
    "    config={\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"architecture\": \"Efficient Net V2_L\",\n",
    "        \"dataset\": \"Custom\",\n",
    "        \"epochs\": epochs,\n",
    "    }\n",
    ")\n",
    "run.watch(model, log='all', log_graph=True)\n",
    "train_model(\n",
    "    model,\n",
    "    final_layer,\n",
    "    trainloader, \n",
    "    testloader, \n",
    "    learning_rate,\n",
    "    weights_decay,\n",
    "    epochs, \n",
    "    train_transferred = False, \n",
    "    run = run, \n",
    "    name=\"pt_no_2\"\n",
    ")\n",
    "run.finish()\n",
    "# retraining the entire model with lower learning rate\n",
    "epochs = 10\n",
    "learning_rate = 0.0001\n",
    "run = wandb.init(\n",
    "    project=\"assignment-3\",\n",
    "    name=f\"enet-b{batch_size}-{learning_rate}-all\",\n",
    "    config={\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"architecture\": \"Efficient Net V2_L\",\n",
    "        \"dataset\": \"Custom\",\n",
    "        \"epochs\": epochs,\n",
    "    }\n",
    ")\n",
    "train_model(\n",
    "    model,\n",
    "    final_layer,\n",
    "    trainloader, \n",
    "    testloader, \n",
    "    learning_rate,\n",
    "    weights_decay,\n",
    "    epochs, \n",
    "    train_transferred = True, \n",
    "    run = run, \n",
    "    name=\"pt_yes_2\"\n",
    ")\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7470c103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # inception_netv3\n",
    "# epochs = 20\n",
    "# learning_rate = 0.001\n",
    "# weights_decay = 0.00001\n",
    "# run = wandb.init(\n",
    "#     project=\"assignment-3\",\n",
    "#     name=f\"inet-{learning_rate}-head_only\",\n",
    "#     config={\n",
    "#         \"learning_rate\": learning_rate,\n",
    "#         \"architecture\": \"Inception Net V3\",\n",
    "#         \"dataset\": \"Custom\",\n",
    "#         \"epochs\": epochs,\n",
    "#     }\n",
    "# )\n",
    "# run.watch(model, log='all', log_graph=True)\n",
    "# train_model(\n",
    "#     model,\n",
    "#     final_layer,\n",
    "#     trainloader, \n",
    "#     testloader, \n",
    "#     learning_rate, \n",
    "#     weights_decay,\n",
    "#     epochs, \n",
    "#     train_transferred = False, \n",
    "#     run = run, \n",
    "#     name=\"pt_no\"\n",
    "# )\n",
    "# run.finish()\n",
    "# # retraining the entire model with lower learning rate\n",
    "# epochs = 20\n",
    "# learning_rate = 0.0001\n",
    "# run = wandb.init(\n",
    "#     project=\"assignment-3\",\n",
    "#     name=f\"inet-{learning_rate}-all\",\n",
    "#     config={\n",
    "#         \"learning_rate\": learning_rate,\n",
    "#         \"architecture\": \"Inception Net V3\",\n",
    "#         \"dataset\": \"Custom\",\n",
    "#         \"epochs\": epochs,\n",
    "#     }\n",
    "# )\n",
    "# train_model(\n",
    "#     model,\n",
    "#     final_layer,\n",
    "#     trainloader, \n",
    "#     testloader, \n",
    "#     learning_rate, \n",
    "#     weights_decay,\n",
    "#     epochs, \n",
    "#     train_transferred = True, \n",
    "#     run = run, \n",
    "#     name=\"pt_yes\"\n",
    "# )\n",
    "# run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d62349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # RESNET152\n",
    "# epochs = 20\n",
    "# learning_rate = 0.001\n",
    "# weights_decay = 0.00001\n",
    "# run = wandb.init(\n",
    "#     project=\"assignment-3\",\n",
    "#     name=f\"resnet-{learning_rate}-head_only\",\n",
    "#     config={\n",
    "#         \"learning_rate\": learning_rate,\n",
    "#         \"architecture\": \"ResNet152\",\n",
    "#         \"dataset\": \"Custom\",\n",
    "#         \"epochs\": epochs,\n",
    "#     }\n",
    "# )\n",
    "# run.watch(model, log='all', log_graph=True)\n",
    "# train_model(\n",
    "#     model,\n",
    "#     final_layer,\n",
    "#     trainloader, \n",
    "#     testloader, \n",
    "#     learning_rate,\n",
    "#     weights_decay,\n",
    "#     epochs, \n",
    "#     train_transferred = False, \n",
    "#     run = run, \n",
    "#     name=\"pt_no\"\n",
    "# )\n",
    "# run.finish()\n",
    "# # retraining the entire model with lower learning rate\n",
    "# epochs = 20\n",
    "# learning_rate = 0.0001\n",
    "# run = wandb.init(\n",
    "#     project=\"assignment-3\",\n",
    "#     name=f\"resnet-{learning_rate}-all\",\n",
    "#     config={\n",
    "#         \"learning_rate\": learning_rate,\n",
    "#         \"architecture\": \"ResNet152\",\n",
    "#         \"dataset\": \"Custom\",\n",
    "#         \"epochs\": epochs,\n",
    "#     }\n",
    "# )\n",
    "# train_model(\n",
    "#     model,\n",
    "#     final_layer,\n",
    "#     trainloader, \n",
    "#     testloader, \n",
    "#     learning_rate,\n",
    "#     weights_decay,\n",
    "#     epochs, \n",
    "#     train_transferred = True, \n",
    "#     run = run, \n",
    "#     name=\"pt_yes\"\n",
    "# )\n",
    "# run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a27a534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if multi gpu is being used convert parallel model to single model\n",
    "# model.load_state_dict(torch.load('./models/enet3_10_parallel.pth'))\n",
    "# torch.save(model.module.state_dict(), './models/en3_10.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0eba0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finaltestloader = DataLoader(test_image_tensor, batch_size = 1, shuffle = False)\n",
    "# model.eval()\n",
    "# predicted = None\n",
    "# with torch.no_grad():\n",
    "#     for idx, test_images in enumerate(finaltestloader):\n",
    "#         outputs = model(test_images)\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "# torch.save(predicted, './predictions.pt')    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
